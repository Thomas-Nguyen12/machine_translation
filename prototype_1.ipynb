{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVx031IUyH1N"
      },
      "source": [
        "# Project Description\n",
        "\n",
        "This project aims to use transformer-based NLP models to accurately translate text between separate languages - namely English, Vietnamese and Indonesian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3TJMrv4yH1P",
        "outputId": "8703c3ac-cbc4-4358-8b89-bbda6908eb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Masking\n",
        "from keras_nlp.layers import TokenAndPositionEmbedding, TransformerEncoder, TransformerDecoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AItvKnAA7XR",
        "outputId": "2b195068-3db8-4938-da81-22bba3df872d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C108r8jayH1P"
      },
      "source": [
        "# Data Import\n",
        "\n",
        "This dataset, taken from kaggle: [English and Indonesian subtitles](https://www.kaggle.com/datasets/greegtitan/english-indonesia-movie-subtitles), representing episode 13 of a tv series entitled: \"\"\n",
        "\n",
        "## Column names\n",
        "\n",
        "1. **id** = the Indonesian translation of subtitle\n",
        "2. **en** = English subtitle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lorTl4sTyH1P"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/english_indonesian_subtitles.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhxIs7YzyH1P"
      },
      "source": [
        "# Data cleaning\n",
        "\n",
        "- data types\n",
        "- duplicates\n",
        "- missing values\n",
        "- unique values\n",
        "- erroeneous values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qez80buHyH1Q",
        "outputId": "ab141cfd-3cb9-4950-c688-82579ef8bff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning text...\n",
            "\n",
            "\n",
            "Calculating maximum phrase length\n"
          ]
        }
      ],
      "source": [
        "# Drop the first row and reset index\n",
        "df.drop([0], axis=0, inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Cleaning text\n",
        "print(\"Cleaning text...\\n\")\n",
        "\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = normalize('NFD', text.lower())\n",
        "        text = re.sub('[^A-Za-z ]+', '', text)\n",
        "    return text\n",
        "\n",
        "def clean_and_prepare_text(text):\n",
        "    text = '[start] ' + clean_text(text) + ' [end]'\n",
        "    return text if isinstance(text, str) else ''  # Return an empty string if text is not a string\n",
        "\n",
        "# Apply the cleaning functions to the DataFrame\n",
        "df['id'] = df['id'].apply(clean_and_prepare_text)\n",
        "df['en'] = df['en'].apply(clean_text)\n",
        "df.head()\n",
        "\n",
        "en = df['en']\n",
        "indonesian = df['id']\n",
        "print(\"\\nCalculating maximum phrase length\")\n",
        "\n",
        "en_max_len = 106\n",
        "id_max_len = 112\n",
        "sequence_len = 112\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB-ucxZgyH1R",
        "outputId": "a624c2dc-103e-4882-c94f-e4ad9a8c02fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max phrase length (English): 106\n",
            "Max phrase length (Indonesian): 112\n",
            "Sequence length: 112\n",
            "\n",
            "Calculating vocabulary size\n",
            "Vocabulary size (English): 272792\n",
            "Vocabulary size (Indonesian): 359908\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'Max phrase length (English): {en_max_len}')\n",
        "print(f'Max phrase length (Indonesian): {id_max_len}')\n",
        "print(f'Sequence length: {sequence_len}\\n')\n",
        "\n",
        "# Filter out non-string values and convert to lowercase\n",
        "en_cleaned = [text.lower() for text in en if isinstance(text, str)]\n",
        "id_cleaned = [text.lower() for text in indonesian if isinstance(text, str)]\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "en_tokenizer = Tokenizer()\n",
        "en_tokenizer.fit_on_texts(en_cleaned)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en_cleaned)\n",
        "en_x = pad_sequences(en_sequences, maxlen=sequence_len, padding='post')\n",
        "\n",
        "id_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
        "id_tokenizer.fit_on_texts(id_cleaned)\n",
        "id_sequences = id_tokenizer.texts_to_sequences(id_cleaned)\n",
        "id_y = pad_sequences(id_sequences, maxlen=sequence_len + 1, padding='post')\n",
        "\n",
        "# Calculate the vocabulary sizes from the tokenizer instances\n",
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "id_vocab_size = len(id_tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Calculating vocabulary size\")\n",
        "print(f'Vocabulary size (English): {en_vocab_size}')\n",
        "print(f'Vocabulary size (Indonesian): {id_vocab_size}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKSTNrqqyH1R",
        "outputId": "11b6d0e4-78d2-4c8e-99ad-c85cb4642044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quickly clearing RAM\n",
            "\n",
            "Model: \"model_1\"\n",
            "________________________________________________________________________________________________________________________\n",
            " Layer (type)                       Output Shape                        Param #     Connected to                        \n",
            "========================================================================================================================\n",
            " encoder_input (InputLayer)         [(None, None)]                      0           []                                  \n",
            "                                                                                                                        \n",
            " token_and_position_embedding (Tok  (None, None, 10)                    2729040     ['encoder_input[0][0]']             \n",
            " enAndPositionEmbedding)                                                                                                \n",
            "                                                                                                                        \n",
            " decoder_input (InputLayer)         [(None, None)]                      0           []                                  \n",
            "                                                                                                                        \n",
            " transformer_encoder (TransformerE  (None, None, 10)                    700         ['token_and_position_embedding[0][0]\n",
            " ncoder)                                                                            ']                                  \n",
            "                                                                                                                        \n",
            " model (Functional)                 (None, None, 359908)                7560348     ['decoder_input[0][0]',             \n",
            "                                                                                     'transformer_encoder[0][0]']       \n",
            "                                                                                                                        \n",
            "========================================================================================================================\n",
            "Total params: 10290088 (39.25 MB)\n",
            "Trainable params: 10290088 (39.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "________________________________________________________________________________________________________________________\n",
            "  1052/118184 [..............................] - ETA: 166:21:15 - loss: 0.8012 - accuracy: 0.9470"
          ]
        }
      ],
      "source": [
        "print(\"Quickly clearing RAM\\n\")\n",
        "gc.collect()\n",
        "\n",
        "# Define model parameters\n",
        "num_heads = 1\n",
        "embed_dim = 10\n",
        "\n",
        "# Define the model architecture\n",
        "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
        "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
        "\n",
        "# Token and Position Embedding layer should be defined elsewhere in the code\n",
        "x_enc = TokenAndPositionEmbedding(en_vocab_size, sequence_len, embed_dim)(encoder_input)\n",
        "encoder_output = TransformerEncoder(embed_dim, num_heads)(x_enc)\n",
        "\n",
        "# Adding Masking to handle padded sequences\n",
        "x_dec = TokenAndPositionEmbedding(id_vocab_size, sequence_len, embed_dim)(decoder_input)\n",
        "masked_x_dec = Masking(mask_value=0)(x_dec)\n",
        "\n",
        "x_dec = TransformerDecoder(embed_dim, num_heads)(masked_x_dec, encoder_output)\n",
        "x_dec = Dropout(0.4)(x_dec)\n",
        "\n",
        "decoder_output = Dense(id_vocab_size, activation='softmax')(x_dec)\n",
        "\n",
        "decoder = Model([decoder_input, encoder_output], decoder_output)\n",
        "decoder_output = decoder([decoder_input, encoder_output])\n",
        "\n",
        "model = Model([encoder_input, decoder_input], decoder_output)\n",
        "\n",
        "# Define the SGD optimizer with a reasonable learning rate\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.99)\n",
        "model.compile(optimizer=sgd_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary(line_length=120)\n",
        "\n",
        "# Assuming en_x and id_y are already defined\n",
        "inputs = {'encoder_input': en_x, 'decoder_input': id_y[:, :-1]}\n",
        "outputs = id_y[:, 1:]\n",
        "\n",
        "batch_size = 50  # Choose an appropriate batch size\n",
        "steps_per_epoch = len(en_x) // batch_size\n",
        "\n",
        "# Fit the model with steps_per_epoch\n",
        "callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "hist = model.fit(inputs, outputs, epochs=1, validation_split=0.2, callbacks=[callback], batch_size=batch_size)\n",
        "\n",
        "# Saving the model\n",
        "model.save(\"english_indonesian_translate_reduced.h5\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g2jEqsYvxRM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}